{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7b09f4ad-f23e-493c-afd4-3bafeb7d59d5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# **PYSPARK INTERVIEW QUESTIONS - ANSH LAMBA**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2b523a1-444f-4442-9f20-99c8dc92b146",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import * \n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Test PySpark Installation\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d13400f3-69d3-4b46-b35d-9d09b23515bc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Q1 While ingesting customer data from an external source, you notice duplicate entries. How would you remove duplicates and retain only the latest entry based on a timestamp column?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fee50763-00a8-430a-a8c7-355ff6714c05",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+\n",
      "|product_id|      date|sales|\n",
      "+----------+----------+-----+\n",
      "|       101|2023-12-01|  100|\n",
      "|       101|2023-12-02|  150|\n",
      "|       102|2023-12-01|  200|\n",
      "|       102|2023-12-02|  250|\n",
      "+----------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"101\", \"2023-12-01\", 100), (\"101\", \"2023-12-02\", 150), \n",
    "        (\"102\", \"2023-12-01\", 200), (\"102\", \"2023-12-02\", 250)]\n",
    "columns = [\"product_id\", \"date\", \"sales\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "685bea72-16aa-4089-9a47-26b6bc911cfe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "41066a18-aaeb-4ecc-92a1-15523f596507",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+\n",
      "|product_id|      date|sales|\n",
      "+----------+----------+-----+\n",
      "|       101|2023-12-02|  150|\n",
      "|       102|2023-12-02|  250|\n",
      "+----------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.withColumn('date',col('date').cast(DateType()))\n",
    "#df=df.withColumn('date',to_date(col('date')))\n",
    "df=df.orderBy('product_id','date',ascending=[1,0]).dropDuplicates(subset=['product_id'])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It keeps only the first occurrence of each unique value in the specified subset column(s).\n",
    "\n",
    "It removes all other rows that have the same value(s) in those columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b359a049-a1d5-49af-9af2-ef3c4fa59d4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>product_id</th><th>date</th><th>sales</th></tr></thead><tbody><tr><td>101</td><td>2023-12-02</td><td>150</td></tr><tr><td>102</td><td>2023-12-02</td><td>250</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "101",
         "2023-12-02",
         150
        ],
        [
         "102",
         "2023-12-02",
         250
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "product_id",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "date",
         "type": "\"date\""
        },
        {
         "metadata": "{}",
         "name": "sales",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "32fc5f0e-259b-4cca-98ec-df5b4fca85ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**2. While processing data from multiple files with inconsistent schemas, you need to merge them into a single DataFrame. How would you handle this inconsistency in PySpark?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "71b7112f-ae75-4498-a476-95e447444881",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Solution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7395ec4d-d9e0-4a0a-b621-6b5a7a033043",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=spark.read.format('parquet')\\\n",
    "                .option('mergeSchema',True)\\\n",
    "                .load('Path to file')\n",
    "\n",
    "#only works for Parquet\n",
    "#ORC (some extent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For CSV and JSON Files, handle manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.read.option(\"header\", True).csv(\"file1.csv\")\n",
    "df2 = spark.read.option(\"header\", True).csv(\"file2.csv\")\n",
    "\n",
    "df1 = spark.read.option(\"multiline\", True).json(\"file1.json\")\n",
    "df2 = spark.read.option(\"multiline\", True).json(\"file2.json\")\n",
    "\n",
    "all_columns = list(set(df1.columns).union(set(df2.columns)))\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "\n",
    "def align_columns(df, all_cols):\n",
    "    for col_name in all_cols:\n",
    "        if col_name not in df.columns:\n",
    "            df = df.withColumn(col_name, lit(None))  # Add missing col with null\n",
    "    return df.select(sorted(all_cols))  # Keep consistent column order\n",
    "\n",
    "df1_aligned = align_columns(df1, all_columns)\n",
    "df2_aligned = align_columns(df2, all_columns)\n",
    "\n",
    "\n",
    "merged_df = df1_aligned.unionByName(df2_aligned)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. What are the key differences between Spark and Hadoop MatpReduce in terms of performance and scalability?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hadoop MapReduce is a framework to process data paralelly\n",
    "\n",
    "Hadoop does the computation by Map and Reduce\n",
    "Map is used to distribute data among nodes and this data is combined at Reduce stage, the issue is the map is writing the intermediate data to disk, then the reduces will read data from disk, so it impact the required resources and time and performance.\n",
    "\n",
    "as spark being work with in-memory it is much better in terms of performance.\n",
    "\n",
    "\"In summary, Spark is preferred for speed and flexibility, especially when in-memory processing is feasible, while MapReduce is more suited to traditional, disk-based batch jobs in resource-constrained environments.\"\n",
    "\n",
    "and hadoop is mostly created for Batch processing, whereas the spark is created for both batch and streaming processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a9c9d12-8fd3-4b0d-a22e-65d495e289f9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**4. You are working with a real-time data pipeline, and you notice missing values in your streaming data Column - Category. How would you handle null or missing values in such a scenario?**\n",
    "\n",
    "**df_stream = spark.readStream.schema(\"id INT, value STRING\").csv(\"path/to/stream\")**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6ce88b1e-125b-4005-800f-3eb90eb1532c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=df.fillna({'Product_id':'N/A'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "36649713-d578-40d3-813a-fdb671484f97",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**5. You need to calculate the total number of actions performed by users in a system. How would you calculate the top 5 most active users based on this information?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "357c3dad-c40d-475d-93a4-be8eea59515e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|user_id|actions|\n",
      "+-------+-------+\n",
      "|  user1|      5|\n",
      "|  user2|      8|\n",
      "|  user3|      2|\n",
      "|  user4|     10|\n",
      "|  user2|      3|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"user1\", 5), (\"user2\", 8), (\"user3\", 2), (\"user4\", 10), (\"user2\", 3)]\n",
    "columns = [\"user_id\", \"actions\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe943241-a1d1-4490-b25a-64fc63491d7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------------+\n",
      "|user_id|total_actions|\n",
      "+-------+-------------+\n",
      "|  user2|           11|\n",
      "|  user4|           10|\n",
      "|  user1|            5|\n",
      "|  user3|            2|\n",
      "+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.groupBy('user_id').agg(sum('actions').alias('total_actions')).orderBy('total_actions',ascending=False).limit(5)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "66886b92-3d5d-4226-978d-78bdcc52fc1d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**6. While processing sales transaction data, you need to identify the most recent transaction for each customer. How would you approach this task?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "de687585-5de0-459a-907d-d2008276d643",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+-----+\n",
      "|customer_id|transaction_date|sales|\n",
      "+-----------+----------------+-----+\n",
      "|      cust1|      2023-12-01|  100|\n",
      "|      cust2|      2023-12-02|  150|\n",
      "|      cust1|      2023-12-03|  200|\n",
      "|      cust2|      2023-12-04|  250|\n",
      "+-----------+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"cust1\", \"2023-12-01\", 100), (\"cust2\", \"2023-12-02\", 150),\n",
    "        (\"cust1\", \"2023-12-03\", 200), (\"cust2\", \"2023-12-04\", 250)]\n",
    "columns = [\"customer_id\", \"transaction_date\", \"sales\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce0a814f-b220-40d4-841d-b46bdcec4590",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------+-----+----+\n",
      "|customer_id|transaction_date|sales|flag|\n",
      "+-----------+----------------+-----+----+\n",
      "|      cust1|      2023-12-03|  200|   1|\n",
      "|      cust2|      2023-12-04|  250|   1|\n",
      "+-----------+----------------+-----+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "df=df.withColumn('transaction_date',col('transaction_date').cast(DateType()))\n",
    "\n",
    "df=df.withColumn('flag',dense_rank().over(Window.partitionBy('customer_id').orderBy(col('transaction_date').desc()))).filter(col('flag') == 1)\n",
    "\n",
    "df.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f96aab33-5aa3-4b7e-ad4d-f89c89bc734e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**7. You need to identify customers who haven’t made any purchases in the last 30 days. How would you filter such customers?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "412dbceb-9797-4957-b808-c1fd9ef91d44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+\n",
      "|customer_id|last_purchase_date|\n",
      "+-----------+------------------+\n",
      "|      cust1|        2025-12-01|\n",
      "|      cust2|        2024-11-20|\n",
      "|      cust3|        2024-11-25|\n",
      "+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"cust1\", \"2025-12-01\"), (\"cust2\", \"2024-11-20\"), (\"cust3\", \"2024-11-25\")]\n",
    "columns = [\"customer_id\", \"last_purchase_date\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8edc9f01-8c86-4c5c-a5d7-f29d866900aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+---------+\n",
      "|customer_id|last_purchase_date|date_diff|\n",
      "+-----------+------------------+---------+\n",
      "|      cust1|        2025-12-01|     -233|\n",
      "|      cust2|        2024-11-20|      143|\n",
      "|      cust3|        2024-11-25|      138|\n",
      "+-----------+------------------+---------+\n",
      "\n",
      "+-----------+------------------+---------+\n",
      "|customer_id|last_purchase_date|date_diff|\n",
      "+-----------+------------------+---------+\n",
      "|      cust2|        2024-11-20|      143|\n",
      "|      cust3|        2024-11-25|      138|\n",
      "+-----------+------------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.withColumn('last_purchase_date',to_date('last_purchase_date'))\n",
    "df.show()\n",
    "df=df.withColumn('date_diff',date_diff(current_date(),'last_purchase_date')).filter(col('date_diff')>30)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3d0c56c6-a481-4524-9810-271f44c6ffb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**8. While analyzing customer reviews, you need to identify the most frequently used words in the feedback. How would you implement this?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6ad1bf1-6476-44f1-bcfd-1c690ff245fd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------------------------+\n",
      "|customer_id|feedback                    |\n",
      "+-----------+----------------------------+\n",
      "|customer1  |The product is great        |\n",
      "|customer2  |Great product, fast delivery|\n",
      "|customer3  |Not bad, could be better    |\n",
      "+-----------+----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"customer1\", \"The product is great\"), (\"customer2\", \"Great product, fast delivery\"), (\"customer3\", \"Not bad, could be better\")]\n",
    "columns = [\"customer_id\", \"feedback\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53e56c14-7e35-4c58-a64f-7334e8388da1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|feedback|wordcount|\n",
      "+--------+---------+\n",
      "|great   |2        |\n",
      "|is      |1        |\n",
      "|the     |1        |\n",
      "|product |1        |\n",
      "|fast    |1        |\n",
      "|delivery|1        |\n",
      "|product,|1        |\n",
      "|could   |1        |\n",
      "|not     |1        |\n",
      "|be      |1        |\n",
      "|bad,    |1        |\n",
      "|better  |1        |\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.withColumn('feedback',lower('feedback')).withColumn('feedback',explode(split('feedback',' ')))\n",
    "df_grp=df.groupBy('feedback').agg(count('feedback').alias('wordcount'))\n",
    "df_grp.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e362d56-5a61-42ed-b9de-ad6353e1d9a2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**9. You need to calculate the cumulative sum of sales over time for each product. How would you approach this?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "935a51d6-04f4-4a95-9a78-c824c5513acd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+\n",
      "|product_id|      date|sales|\n",
      "+----------+----------+-----+\n",
      "|  product1|2023-12-01|  100|\n",
      "|  product2|2023-12-02|  200|\n",
      "|  product1|2023-12-03|  150|\n",
      "|  product2|2023-12-04|  250|\n",
      "+----------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"product1\", \"2023-12-01\", 100), (\"product2\", \"2023-12-02\", 200),\n",
    "        (\"product1\", \"2023-12-03\", 150), (\"product2\", \"2023-12-04\", 250)]\n",
    "columns = [\"product_id\", \"date\", \"sales\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a093880a-0034-4f15-b5ec-330a707e931b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+------+\n",
      "|product_id|      date|sales|CumSum|\n",
      "+----------+----------+-----+------+\n",
      "|  product1|2023-12-01|  100|   100|\n",
      "|  product1|2023-12-03|  150|   250|\n",
      "|  product2|2023-12-02|  200|   200|\n",
      "|  product2|2023-12-04|  250|   450|\n",
      "+----------+----------+-----+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.withColumn('date',to_date('date'))\n",
    "df=df.withColumn('CumSum',sum('sales').over(Window.partitionBy('product_id').orderBy('date')))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cea5ed3-0158-4244-b0b6-112adc9ac72a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**10. While preparing a data pipeline, you notice some duplicate rows in a dataset. How would you remove the duplicates without affecting the original order?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0a31c276-2e9d-4ea2-8cc0-94946d7a581a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "| John| 25|\n",
      "| Jane| 30|\n",
      "| John| 25|\n",
      "|Alice| 22|\n",
      "+-----+---+\n",
      "\n",
      "+-----+---+\n",
      "| name|age|\n",
      "+-----+---+\n",
      "| John| 25|\n",
      "| Jane| 30|\n",
      "|Alice| 22|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"John\", 25), (\"Jane\", 30), (\"John\", 25), (\"Alice\", 22)]\n",
    "columns = [\"name\", \"age\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "90ada39e-6a44-47df-817b-0c6f3431e57f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+-------+\n",
      "| name|age|rowflag|\n",
      "+-----+---+-------+\n",
      "|Alice| 22|      1|\n",
      "| Jane| 30|      1|\n",
      "| John| 25|      1|\n",
      "+-----+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.withColumn('rowflag',row_number().over(Window.partitionBy('name').orderBy('age'))).filter(col('rowflag')==1)\n",
    "df.show()\n",
    "\n",
    "#df.distinct.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dea052e0-34e1-469c-98ff-c98d49862c15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**11. You are working with user activity data and need to calculate the average session duration per user. How would you implement this?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d52e5118-b9a9-418c-9d44-3bae3a170c84",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+--------+\n",
      "|user_id|session_date|duration|\n",
      "+-------+------------+--------+\n",
      "|  user1|  2023-12-01|      50|\n",
      "|  user1|  2023-12-02|      60|\n",
      "|  user2|  2023-12-01|      45|\n",
      "|  user2|  2023-12-03|      75|\n",
      "+-------+------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"user1\", \"2023-12-01\", 50), (\"user1\", \"2023-12-02\", 60), \n",
    "        (\"user2\", \"2023-12-01\", 45), (\"user2\", \"2023-12-03\", 75)]\n",
    "columns = [\"user_id\", \"session_date\", \"duration\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd8dee7e-9bab-4472-86c5-0473e15a5edc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+\n",
      "|user_id|avg_dur|\n",
      "+-------+-------+\n",
      "|  user1|   55.0|\n",
      "|  user2|   60.0|\n",
      "+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.groupBy('user_id').agg(avg('duration').alias('avg_dur'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "74fe6db5-82ac-431b-b94a-47a9ed3adcfa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**12. While analyzing sales data, you need to find the product with the highest sales for each month. How would you accomplish this?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2f9f751-f571-4e6c-a20e-b5e6637553c5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----------+-----+\n",
      "|product_id|      date|sales|\n",
      "+----------+----------+-----+\n",
      "|  product1|2023-12-01|  100|\n",
      "|  product2|2023-12-01|  150|\n",
      "|  product1|2023-12-02|  200|\n",
      "|  product2|2023-12-02|  250|\n",
      "+----------+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"product1\", \"2023-12-01\", 100), (\"product2\", \"2023-12-01\", 150), \n",
    "        (\"product1\", \"2023-12-02\", 200), (\"product2\", \"2023-12-02\", 250)]\n",
    "columns = [\"product_id\", \"date\", \"sales\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e632929b-c9a2-43a2-b31c-0451f493008f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----------+-----+-------+\n",
      "|date|product_id|sales|ranking|\n",
      "+----+----------+-----+-------+\n",
      "|  12|  product2|  400|      1|\n",
      "+----+----------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import col\n",
    "df=df.withColumn('date',to_date('date'))\n",
    "\n",
    "df=df.withColumn('date',month('date')).groupBy('date','product_id').agg(sum('sales').alias('sales'))\n",
    "df=df.withColumn('ranking',dense_rank().over(Window.partitionBy('date').orderBy(col('sales').desc()))).filter(col('ranking')==1)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the SparkContext in PySpark?\\\n",
    "Explain Spark Architecture?\\\n",
    "What is difference between RDD,Dataframe and Dataset?\\\n",
    "What is QUERY Optimization?\\\n",
    "Tell about SPARK SESSION?\\\n",
    "Difference between Wide Transformatons and Narrow Transformations?\\\n",
    "What is the use of COALESCE() and Repartition()?\\\n",
    "When to use CACHE() and PERSIST()? What's the difference?\\\n",
    "What is the importance of PARTITIONS in pyspark?\\\n",
    "What is MPP?\\\n",
    "What are broadcast variables, and why are they used?\\\n",
    "What is difference between df.show() and df.collect()\\\n",
    "What is LAZY EVALUATION in Pyspark?\\\n",
    "What are the Advantages of Delta Lake over traditional file formats?\\\n",
    "What happens when a Pyspark job runs of out memory?(DOOM - Drivr out of memory or Executor Out of Memory)\\\n",
    "What is Skewed?\\\n",
    "What is AQE(Adaptive Query Executions) in PySpark? Why is it useful?\\\n",
    "What is salting?\\\n",
    "How would you handle skewed data in pyspark?\\\n",
    "What is broadcast join, and when you use it?\\\n",
    "What is spill in spark, why does it happen?\\\n",
    "What are Delta lake's time travel features, and how do they work?\\\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce8d0a6f-c11f-49c6-bbe6-86a79b0ddf66",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**13. You are working with a large Delta table that is frequently updated by multiple users. The data is stored in partitions, and sometimes updates can cause inconsistent reads due to concurrent transactions. How would you ensure ACID compliance and avoid data corruption in PySpark?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "58b531a8-3ac6-4b09-b951-84f33625c89c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#New Data\n",
    "\n",
    "df=spark.read.format('parquet').load('path')\n",
    "\n",
    "frome delta.table import DeltaTable\n",
    "\n",
    "delta_tbl=DeltaTable.forPath('path')\n",
    "\n",
    "delta_tbl.alias('trg').merge(df.alias('src'),\"src.id==trg.id\")\\\n",
    "                    .whenNotMatchedInsertAll() \\\n",
    "                    .whenMatchedUpdateAll() \\\n",
    "                    .execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85e69a4f-8d5a-4869-8077-8eb98549193c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**14. You need to process a large dataset stored in PARQUET format and ensure that all columns have the right schema (Almost). How would you do this?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ca277d01-7f03-4b34-b60b-03f2c3feb7e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=spark.read.format('parquet')\\\n",
    "            .option('inferSchema',True)\\\n",
    "            .load('path')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "170f33b8-6c07-4693-8ba5-6a7a1d2d00ff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**15. You are reading a CSV file and need to handle corrupt records gracefully by skipping them. How would you configure this in PySpark?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "762b0800-277b-4d0e-9351-5e35081d6a7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=spark.read.format('csv')\\\n",
    "                .option(\"mode\",'DROPMALFORMED')\\\n",
    "                .load('staging location')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "471f2e09-637d-47c0-bed7-31d5ebd09893",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**22. You have a dataset containing the names of employees and their departments. You need to find the department with the most employees.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76ab8fc1-63a8-4e2b-94ae-1c9972fdd986",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+\n",
      "|employee_name| department|\n",
      "+-------------+-----------+\n",
      "|        Alice|         HR|\n",
      "|          Bob|    Finance|\n",
      "|      Charlie|         HR|\n",
      "|        David|Engineering|\n",
      "|          Eve|    Finance|\n",
      "+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Alice\", \"HR\"), (\"Bob\", \"Finance\"), (\"Charlie\", \"HR\"), (\"David\", \"Engineering\"), (\"Eve\", \"Finance\")]\n",
    "columns = [\"employee_name\", \"department\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1a0ed6d5-4fda-450c-b572-a74dea5cd142",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+\n",
      "| department|total_employees|\n",
      "+-----------+---------------+\n",
      "|         HR|              2|\n",
      "|    Finance|              2|\n",
      "|Engineering|              1|\n",
      "+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.groupBy('department').agg(count('employee_name').alias('total_employees')).sort('total_employees',ascending=False)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0db0a70a-aa4a-454d-aa7e-6c4d41ccd971",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**23. While processing sales data, you need to classify each transaction as either 'High' or 'Low' based on its amount. How would you achieve this using a when condition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f36aeace-6389-42fd-bdb5-8c5211ce3fa1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|product_id|sales|\n",
      "+----------+-----+\n",
      "|  product1|  100|\n",
      "|  product2|  300|\n",
      "|  product3|   50|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"product1\", 100), (\"product2\", 300), (\"product3\", 50)]\n",
    "columns = [\"product_id\", \"sales\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "10f160e6-eac4-413d-bdcd-6faff5957d28",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+---------+\n",
      "|product_id|sales|price_cat|\n",
      "+----------+-----+---------+\n",
      "|  product1|  100|     high|\n",
      "|  product2|  300|     high|\n",
      "|  product3|   50|      low|\n",
      "+----------+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.withColumn('price_cat',when(col('sales')>50,\"high\").otherwise(\"low\"))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4ba5720a-72b0-4598-ab0e-e20daca0cb8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**24. While analyzing a large dataset, you need to create a new column that holds a timestamp of when the record was processed. How would you implement this and what can be the best USE CASE?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b6e90a9-dabc-4efd-b08a-ebcd18cc0775",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|product_id|sales|\n",
      "+----------+-----+\n",
      "|  product1|  100|\n",
      "|  product2|  200|\n",
      "|  product3|  300|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"product1\", 100), (\"product2\", 200), (\"product3\", 300)]\n",
    "columns = [\"product_id\", \"sales\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b4f7adb2-93c9-4e91-8d2f-ea54987c55f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------------------------+\n",
      "|product_id|sales|processed_time            |\n",
      "+----------+-----+--------------------------+\n",
      "|product1  |100  |2025-04-16 22:26:57.570872|\n",
      "|product2  |200  |2025-04-16 22:26:57.570872|\n",
      "|product3  |300  |2025-04-16 22:26:57.570872|\n",
      "+----------+-----+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.withColumn(\"processed_time\",current_timestamp())\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4afedb5a-b14e-43b8-b128-c6945ac1b7ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**25. You need to register this PySpark DataFrame as a temporary SQL object and run a query on it. How would you achieve this?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e808204e-3dfa-4711-9c1e-3c41b6dae9e0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|product_id|sales|\n",
      "+----------+-----+\n",
      "|  product1|  100|\n",
      "|  product2|  200|\n",
      "|  product3|  300|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"product1\", 100), (\"product2\", 200), (\"product3\", 300)]\n",
    "columns = [\"product_id\", \"sales\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4834f1fc-813e-4b37-95e7-a447034b25e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('tempsqldf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|product_id|sales|\n",
      "+----------+-----+\n",
      "|  product1|  100|\n",
      "|  product2|  200|\n",
      "|  product3|  300|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from tempsqldf\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cd45ba64-911e-42ad-a545-fd1a1a734020",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**26. You need to register this PySpark DataFrame as a temporary SQL object and run a query on it (FROM DIFFERENT NOTEBOOKS AS WELL)?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64c23351-bcdf-403f-8fcb-ff12a094f154",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceGlobalTempView(\"globalview\")\n",
    "select * from global_temp.globalview;\n",
    "#use global_temp keyword"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "12bfd204-4c17-4e18-87fb-a96518d7416a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**27. You need to query data from a PySpark DataFrame using SQL, but the data includes a nested structure. How would you flatten the data for easier querying?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "207b6f91-f20e-41a5-81c3-6bec10435773",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------------------------+\n",
      "|product_id|product_info                 |\n",
      "+----------+-----------------------------+\n",
      "|product1  |{price -> 100, quantity -> 2}|\n",
      "|product2  |{price -> 200, quantity -> 3}|\n",
      "+----------+-----------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"product1\", {\"price\": 100, \"quantity\": 2}), \n",
    "        (\"product2\", {\"price\": 200, \"quantity\": 3})]\n",
    "columns = [\"product_id\", \"product_info\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f84176f4-4723-4b01-abef-531e76b0a730",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.select(\"product_id\",\"product_info.price\",\"product_info.quantity\").createOrReplaceTempView(\"flatview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+--------+\n",
      "|product_id|price|quantity|\n",
      "+----------+-----+--------+\n",
      "|  product1|  100|       2|\n",
      "|  product2|  200|       3|\n",
      "+----------+-----+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from flatview\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2b2cb267-05c9-430d-bda1-ec1367502a8d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**28. You are ingesting data from an external API in JSON format where the schema is inconsistent. How would you handle this situation to ensure a robust pipeline?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "89ce7dee-017a-47a2-91c0-f8f9e8c15f26",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df=spark.read.format(\"json\").option(\"mergeSchema\",True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7069dce2-ec80-4bf3-a914-762902e2ed88",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**29. While reading data from Parquet, you need to optimize performance by partitioning the data based on a column. How would you implement this?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2db9363c-78d1-42bd-b9a9-df732d30ec0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format(\"parquet\").mode(\"append\").partitionBy(\"category\").save(\"location\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4159e62e-0e57-4175-9933-b78981e64e4a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**30. You are working with a large dataset in Parquet format and need to ensure that the data is written in an optimized manner with proper compression. How would you accomplish this?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "394a5d85-6324-404c-b3bf-58ebd24896ee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df.write.format('parquet').option(\"compression\",\"snappy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd4673ec-b00c-4769-af90-f82777123166",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**31. Your company uses a large-scale data pipeline that reads from Delta tables and processes data using complex aggregations. However, performance is becoming an issue due to the growing dataset size. How would you optimize the performance of the pipeline?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7ae2f8b2-c014-4c8f-885a-5dc58f829580",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "OPTIMIZE tabledelta ZORDER BY ('order_date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aac3f322-a244-4b82-808a-c5c4e5e8a4b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**43. You are processing sales data. Group by product categories and create a list of all product names in each category.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fb72285-393b-4dae-a1dd-bd13f5116dba",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|   category|   product|\n",
      "+-----------+----------+\n",
      "|Electronics|    Laptop|\n",
      "|Electronics|Smartphone|\n",
      "|  Furniture|     Chair|\n",
      "|  Furniture|     Table|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"Electronics\", \"Laptop\"), (\"Electronics\", \"Smartphone\"), (\"Furniture\", \"Chair\"), (\"Furniture\", \"Table\")]\n",
    "columns = [\"category\", \"product\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d766db09-a3c9-4d91-a66e-64487bbdd86e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|   category|            products|\n",
      "+-----------+--------------------+\n",
      "|Electronics|[Laptop, Smartphone]|\n",
      "|  Furniture|      [Chair, Table]|\n",
      "+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.groupBy('category').agg(collect_list('product').alias('products'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d2ca78ee-707f-43f5-8e8f-59734fa84c71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**44. You are analyzing orders. Group by customer IDs and list all unique product IDs each customer purchased.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86e91e98-03b5-4e19-9490-409bd1b16427",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----------+\n",
      "|customer_id|product_id|\n",
      "+-----------+----------+\n",
      "|        101|      P001|\n",
      "|        101|      P002|\n",
      "|        102|      P001|\n",
      "|        101|      P001|\n",
      "+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(101, \"P001\"), (101, \"P002\"), (102, \"P001\"), (101, \"P001\")]\n",
    "columns = [\"customer_id\", \"product_id\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6df067b-9db9-4ade-875f-3108e81b3376",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+\n",
      "|customer_id|unique_products|\n",
      "+-----------+---------------+\n",
      "|        101|   [P002, P001]|\n",
      "|        102|         [P001]|\n",
      "+-----------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.groupBy('customer_id').agg(collect_set('product_id').alias('unique_products'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7fba5125-db94-4e77-a24b-d8a0cfd4e12e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**45. For customer records, combine first and last names only if the email address exists.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ff10813-ebd2-4f7d-8218-95b073dc4d14",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+\n",
      "|first_name|last_name|               email|\n",
      "+----------+---------+--------------------+\n",
      "|      John|      Doe|john.doe@example.com|\n",
      "|      Jane|    Smith|                NULL|\n",
      "+----------+---------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [(\"John\", \"Doe\", \"john.doe@example.com\"), (\"Jane\", \"Smith\", None)]\n",
    "columns = [\"first_name\", \"last_name\", \"email\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e25ccb2b-a6e0-45a3-97d4-d5896875652b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------+--------------------+--------+\n",
      "|first_name|last_name|               email|fullname|\n",
      "+----------+---------+--------------------+--------+\n",
      "|      John|      Doe|john.doe@example.com|John.Doe|\n",
      "|      Jane|    Smith|                NULL|    NULL|\n",
      "+----------+---------+--------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.withColumn('fullname',when(col('email').isNotNull(),concat_ws(\".\",'first_name','last_name')).otherwise(None))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7420aad9-8291-49e8-84b6-6d624d8933a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**46. You have a DataFrame containing customer IDs and a list of their purchased product IDs. Calculate the number of products each customer has purchased.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "93b10669-bed9-4a58-a9a5-9490f2fa140d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+\n",
      "|customer_id|         product_ids|\n",
      "+-----------+--------------------+\n",
      "|          1|[prod1, prod2, pr...|\n",
      "|          2|             [prod4]|\n",
      "|          3|      [prod5, prod6]|\n",
      "+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, [\"prod1\", \"prod2\", \"prod3\"]),\n",
    "    (2, [\"prod4\"]),\n",
    "    (3, [\"prod5\", \"prod6\"]),\n",
    "]\n",
    "myschema = \"customer_id INT ,product_ids array<STRING>\"\n",
    "\n",
    "df = spark.createDataFrame(data, myschema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59e1616e-233a-4252-9553-c781e26df046",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+----------+\n",
      "|customer_id|         product_ids|no_of_prod|\n",
      "+-----------+--------------------+----------+\n",
      "|          1|[prod1, prod2, pr...|         3|\n",
      "|          2|             [prod4]|         1|\n",
      "|          3|      [prod5, prod6]|         2|\n",
      "+-----------+--------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.withColumn('no_of_prod',size(col('Product_ids')))\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5c009ab5-aa34-422c-a024-5a5c3d634e6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**47. You have employee IDs of varying lengths. Ensure all IDs are 6 characters long by padding with leading zeroes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8958ab58-6618-4ba1-bf8a-730c53a57ea8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|employee_id|\n",
      "+-----------+\n",
      "|          1|\n",
      "|        123|\n",
      "|       4567|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"1\",),\n",
    "    (\"123\",),\n",
    "    (\"4567\",),\n",
    "]\n",
    "schema = [\"employee_id\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e0c52074-0093-40f7-89a0-0805bd95c85c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|employee_id|\n",
      "+-----------+\n",
      "|     000001|\n",
      "|     000123|\n",
      "|     004567|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.withColumn(\"employee_id\",lpad(col('employee_id'),6,'0'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9d12198b-4d55-4090-9c56-b7d8101e5571",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**48. You need to validate phone numbers by checking if they start with \"91\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c22729f4-f141-44c8-9748-c8437f68660d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|phone_number|\n",
      "+------------+\n",
      "|911234567890|\n",
      "|811234567890|\n",
      "|912345678901|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"911234567890\",),\n",
    "    (\"811234567890\",),\n",
    "    (\"912345678901\",),\n",
    "]\n",
    "schema = [\"phone_number\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "51aa4c07-8f0a-4ae5-87c5-e080cc18206b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|phone_number|\n",
      "+------------+\n",
      "|911234567890|\n",
      "|912345678901|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.filter(substring(col('phone_number'),1,2)==\"91\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bf65eb8-4376-4bdd-a4a9-b9c7ebaf1344",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**49. You have a dataset with courses taken by students. Calculate the average number of courses per student.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b719eb07-fd88-4822-951a-640cc0f24ac1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------------------+\n",
      "|student_id|           courses|\n",
      "+----------+------------------+\n",
      "|         1|   [Math, Science]|\n",
      "|         2|         [History]|\n",
      "|         3|[Art, PE, Biology]|\n",
      "+----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (1, [\"Math\", \"Science\"]),\n",
    "    (2, [\"History\"]),\n",
    "    (3, [\"Art\", \"PE\", \"Biology\"]),\n",
    "]\n",
    "schema = [\"student_id\", \"courses\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce0ae4b6-7cd6-47e5-a0b8-55cecd6835ca",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+\n",
      "|avg(course_size)|\n",
      "+----------------+\n",
      "|             2.0|\n",
      "+----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.withColumn(\"course_size\",size('courses')).groupBy().agg(avg('course_size'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b7a19420-1b74-4a69-beb0-68780450d8d4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**50. You have a dataset with primary and secondary contact numbers. Use the primary number if available; otherwise, use the secondary number.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c641a064-be2f-4057-bb0e-f331cfef2628",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------+\n",
      "|primary_contact|secondary_contact|\n",
      "+---------------+-----------------+\n",
      "|           NULL|       1234567890|\n",
      "|     9876543210|             NULL|\n",
      "|     7894561230|       4567891230|\n",
      "+---------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (None, \"1234567890\"),\n",
    "    (\"9876543210\", None),\n",
    "    (\"7894561230\", \"4567891230\"),\n",
    "]\n",
    "schema = [\"primary_contact\", \"secondary_contact\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a96c8594-4b1e-4c0b-9f75-26d5af12929f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+-----------------+----------+\n",
      "|primary_contact|secondary_contact|   contact|\n",
      "+---------------+-----------------+----------+\n",
      "|           NULL|       1234567890|1234567890|\n",
      "|     9876543210|             NULL|9876543210|\n",
      "|     7894561230|       4567891230|7894561230|\n",
      "+---------------+-----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.withColumn(\"contact\",coalesce(col('primary_contact'),col('secondary_contact')))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cddb992d-372b-48dd-b378-4b04a936393a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**51. You are categorizing product codes based on their lengths. If the length is 5, label it as \"Standard\"; otherwise, label it as \"Custom\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a2affef-43d3-4371-b212-14a77475abd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|product_code|\n",
      "+------------+\n",
      "|       prod1|\n",
      "|      prd234|\n",
      "|      pr9876|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"prod1\",),\n",
    "    (\"prd234\",),\n",
    "    (\"pr9876\",),\n",
    "]\n",
    "schema = [\"product_code\"]\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "298b32c1-ec0d-4bbf-b647-1886139cb6a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+---------+\n",
      "|product_code|code_flag|\n",
      "+------------+---------+\n",
      "|       prod1| standard|\n",
      "|      prd234|   custom|\n",
      "|      pr9876|   custom|\n",
      "+------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df=df.withColumn(\"code_flag\",when(length(col('product_code'))==5,\"standard\").otherwise(\"custom\"))\n",
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 65213562212438,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "PySpark Interview",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
